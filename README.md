# ğŸ§  Dotnet Ollama AI ChatBot Project

An AI-powered chatbot web application built using **ASP.NET Core (.NET 9) Web Api**, and **Ollama** for local large language models (LLMs). Supports real-time streaming, Docker deployment, and `.zip` export of chat history.

---

## ğŸš€ Features

- âœ… Chat with local LLMs via **Ollama** (e.g., `llama3`, `mistral`, etc.)
- ğŸ” **Real-time streaming** support with Server-Sent Events (SSE)
- ğŸ“¦ Export chats as `.zip` with full transcript and metadata
- ğŸ³ **Docker-ready** for easy containerized deployment
- ğŸ“œ Clean, extensible architecture with modern .NET 9 practices

---

## ğŸ› ï¸ Tech Stack

| Layer         | Technology                            |
|---------------|----------------------------------------|
| Backend       | ASP.NET Core (.NET 9) Web API          |
| AI Model      | [Ollama](https://ollama.com)           |
| Streaming     | Server-Sent Events (SSE)               |
| Deployment    | Docker                                 |

---

## ğŸ“¸ UI Preview

> ğŸ“· _Add screenshots here for better visual reference._

---

## âš™ï¸ Getting Started

### Prerequisites

- [.NET 9 SDK](https://dotnet.microsoft.com/download/dotnet/9.0)
- [Ollama](https://ollama.com) installed and running locally
- Docker (optional for containerized deployment)
- After you installed ollama on your local, please run: **ollama run llama3** and you must install llama3 llm model.

---

### ğŸ”§ Run Locally

1. **Clone the repo**
   ```bash
   git clone https://github.com/mustafaalkan64/DotnetOllamaAIChatBotProject.git
   cd DotnetOllamaAIChatBotProject
   curl [x](http://localhost:5000/api/chat/ask)Run Ollama locally

bash
Kopyala
DÃ¼zenle
ollama run llama3
Launch the app

bash
Kopyala
DÃ¼zenle
dotnet run --project src/DotnetOllamaAIChatBotProject
Navigate to:

bash
Kopyala
DÃ¼zenle
https://localhost:5001 or http://localhost:5000
ğŸ³ Run with Docker
Build and run the container:

bash
Kopyala
DÃ¼zenle
docker build -t dotnet-ollama-chat .
docker run -p 5000:80 dotnet-ollama-chat
Ensure Ollama is running locally on the host machine.

ğŸ“ Project Structure
bash
Kopyala
DÃ¼zenle
/src
  â””â”€â”€ DotnetOllamaAIChatBotProject
       â”œâ”€â”€ Controllers
       â”œâ”€â”€ Services
       â”œâ”€â”€ Models
       â””â”€â”€ Program.cs
/wwwroot
  â””â”€â”€ index.html (Minimal HTML UI)
