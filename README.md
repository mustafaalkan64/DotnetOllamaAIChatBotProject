# üß† Dotnet Ollama AI ChatBot Project

An AI-powered chatbot web application built using **ASP.NET Core (.NET 9) Web Api**, and **Ollama** for local large language models (LLMs). Supports real-time streaming, Docker deployment, and `.zip` export of chat history.

---

## üöÄ Features

- ‚úÖ Chat with local LLMs via **Ollama** (e.g., `llama3`, `mistral`, etc.)
- üîÅ **Real-time streaming** support with Server-Sent Events (SSE)
- üì¶ Export chats as `.zip` with full transcript and metadata
- üê≥ **Docker-ready** for easy containerized deployment
- üìú Clean, extensible architecture with modern .NET 9 practices

---

## üõ†Ô∏è Tech Stack

| Layer         | Technology                            |
|---------------|----------------------------------------|
| Backend       | ASP.NET Core (.NET 9) Web API          |
| AI Model      | [Ollama](https://ollama.com)           |
| Streaming     | Server-Sent Events (SSE)               |
| Deployment    | Docker                                 |

---

## üì∏ UI Preview

> üì∑ _Add screenshots here for better visual reference._

---

## ‚öôÔ∏è Getting Started

### Prerequisites

- [.NET 9 SDK](https://dotnet.microsoft.com/download/dotnet/9.0)
- [Ollama](https://ollama.com) installed and running locally
- Docker (optional for containerized deployment)
- After you installed ollama on your local, please run: **ollama run llama3** and you must install llama3 llm model.

---

### üîß Run Locally

1. **Clone the repo**
   ```bash
   git clone https://github.com/mustafaalkan64/DotnetOllamaAIChatBotProject.git
   cd DotnetOllamaAIChatBotProject
   curl http://localhost:5000/api/chat/ask \
  -d '{
    "model": "llama3",
    "messages": [
      {
        "role": "user",
        "content": "Explain quantum computing simply."
      }
    ],
    "stream": true
  }'

